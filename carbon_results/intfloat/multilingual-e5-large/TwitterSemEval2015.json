{
  "dataset_revision": "70970daeab8776df92f5ea462b6173c0b46fd2d1",
  "mteb_dataset_name": "TwitterSemEval2015",
  "mteb_version": "1.2.0",
  "test": {
    "co2_emissions": 0.000802617352282542,
    "cos_sim": {
      "accuracy": 0.8627287357692078,
      "accuracy_threshold": 0.9026072025299072,
      "ap": 0.7420638666787291,
      "f1": 0.6898664379365272,
      "f1_threshold": 0.8904680013656616,
      "precision": 0.6440173873255548,
      "recall": 0.7427440633245382
    },
    "dot": {
      "accuracy": 0.8627287357692078,
      "accuracy_threshold": 0.9026072025299072,
      "ap": 0.7420649392959047,
      "f1": 0.6898664379365272,
      "f1_threshold": 0.8904680013656616,
      "precision": 0.6440173873255548,
      "recall": 0.7427440633245382
    },
    "euclidean": {
      "accuracy": 0.8627287357692078,
      "accuracy_threshold": 0.4413452744483948,
      "ap": 0.7420643712079263,
      "f1": 0.6898664379365272,
      "f1_threshold": 0.46804279088974,
      "precision": 0.6440173873255548,
      "recall": 0.7427440633245382
    },
    "evaluation_time": 15.66,
    "manhattan": {
      "accuracy": 0.8620134708231507,
      "accuracy_threshold": 11.290937423706055,
      "ap": 0.7404797933992147,
      "f1": 0.6902846163083944,
      "f1_threshold": 11.980008125305176,
      "precision": 0.6334582323121005,
      "recall": 0.758311345646438
    },
    "max": {
      "accuracy": 0.8627287357692078,
      "ap": 0.7420649392959047,
      "f1": 0.6902846163083944
    }
  }
}